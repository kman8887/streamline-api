{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 'sys.path' is a list of absolute path strings\n",
    "sys.path.append('/Users/garethmiskimmin/streamline-app/api')\n",
    "import user_movie_interactions.user_movie_interaction_service as user_movie_interaction_service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def __get_synthetic_rating(row):\n",
    "    if row[\"interaction_type\"] == \"RATING\":\n",
    "        return float(row[\"rating\"])\n",
    "    elif row[\"interaction_type\"] == \"LIKE\":\n",
    "        return 8.5\n",
    "    elif row[\"interaction_type\"] == \"WATCHED\":\n",
    "        return 6.5\n",
    "    return None\n",
    "\n",
    "df_external = df_external = pd.read_csv(\"../data/external_interactions_test.csv\")\n",
    "df_internal = pd.DataFrame(\n",
    "    user_movie_interaction_service.get_all_user_interactions()\n",
    ")\n",
    "\n",
    "df_all = pd.concat([df_internal, df_external], ignore_index=True)\n",
    "\n",
    "df_all['user_id'] = df_all['user_id'].astype(str)\n",
    "df_all['movie_id'] = df_all['movie_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do prediction stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"rating\"] = df_all.apply(__get_synthetic_rating, axis=1)\n",
    "df_all = df_all.dropna(subset=[\"rating\"])\n",
    "\n",
    "ratings_matrix = (\n",
    "    df_all.pivot_table(index=\"user_id\", columns=\"movie_id\", values=\"rating\")\n",
    "    .astype(float)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "user_features = svd.fit_transform(ratings_matrix)\n",
    "movie_features = svd.components_\n",
    "\n",
    "# Predict ratings (dot product)\n",
    "predicted_ratings = np.dot(user_features, movie_features)\n",
    "predicted_df = pd.DataFrame(\n",
    "    predicted_ratings, index=ratings_matrix.index, columns=ratings_matrix.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "ratings_matrix = df_all.pivot_table(\n",
    "        index=\"user_id\", columns=\"movie_id\", values=\"rating\"\n",
    ").astype(float).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "user_features = svd.fit_transform(ratings_matrix)\n",
    "movie_features = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings (dot product)\n",
    "predicted_ratings = np.dot(user_features, movie_features)\n",
    "predicted_df = pd.DataFrame(\n",
    "    predicted_ratings, index=ratings_matrix.index, columns=ratings_matrix.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save with partioning\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as py_dataset\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "table = pa.Table.from_pandas(predicted_df)\n",
    "path = 'parquet_predictions'\n",
    "if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        \n",
    "pq.write_to_dataset(\n",
    "    table,\n",
    "    root_path=\"parquet_predictions\",\n",
    "    partition_cols=[\"user_id\"],  # ← partition by user\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 9\n",
    "\n",
    "dataset = py_dataset.dataset(f\"parquet_predictions/user_id={user_id}\", format=\"parquet\")\n",
    "table: pa.Table = dataset.to_table()\n",
    "new_predicted_df: pd.DataFrame = table.to_pandas()\n",
    "top10 = (\n",
    "    new_predicted_df.iloc[0].sort_values(ascending=False).head(10).index.tolist()\n",
    ")\n",
    "\n",
    "something = new_predicted_df.iloc[0]\n",
    "\n",
    "movie_score_series = something[\"6798244c6243f72901adb47f\"]\n",
    "\n",
    "movie_score = new_predicted_df.iloc[0][\"6798244c6243f72901adb47f\"]\n",
    "\n",
    "# dataset = py_dataset.dataset(\"parquet_predictions\", format=\"parquet\")\n",
    "\n",
    "# # Filter just a specific user_id\n",
    "# user_id = \"9\"\n",
    "\n",
    "# # Query only that partition (fast!)\n",
    "# table = dataset.to_table(filter=(py_dataset.field(\"user_id\") == user_id))\n",
    "# df_user = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = (\n",
    "        predicted_df.loc['9'].sort_values(ascending=False).head(10).index.tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# 'sys.path' is a list of absolute path strings\n",
    "sys.path.append('/Users/garethmiskimmin/streamline-app/api')\n",
    "from user_movie_interactions import user_movie_interaction_service\n",
    "\n",
    "\n",
    "INTERACTION_WEIGHTS = {\n",
    "    \"RATING\": lambda row: row[\"rating\"],  # use actual rating value\n",
    "    \"LIKE\": lambda row: 1.5,\n",
    "    \"WATCHED\": lambda row: 1.0,\n",
    "    \"REVIEW\": lambda row: 1.2,\n",
    "}\n",
    "\n",
    "def create_ratings_matrix() -> pd.DataFrame:\n",
    "    df_external = df_external = pd.read_csv(\"../data/external_interactions_test.csv\")\n",
    "    df_internal = pd.DataFrame(\n",
    "        user_movie_interaction_service.get_all_user_interactions()\n",
    "    )\n",
    "\n",
    "    df_all = pd.concat([df_internal, df_external], ignore_index=True)\n",
    "\n",
    "    df_all[\"user_id\"] = df_all[\"user_id\"].astype(str)\n",
    "    df_all[\"movie_id\"] = df_all[\"movie_id\"].astype(str)\n",
    "\n",
    "    df_all[\"interaction_score\"] = df_all.apply(__compute_interaction_score, axis=1)\n",
    "    df_all[\"interaction_score\"] = df_all[\"interaction_score\"].astype(float)\n",
    "\n",
    "    df_all = __add_time_decay(df_all)\n",
    "    \n",
    "    df_grouped = (\n",
    "        df_all.groupby([\"user_id\", \"movie_id\"])[\"adjusted_score\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"adjusted_score\": \"final_score\"})\n",
    "    )\n",
    "\n",
    "    df_grouped = __normalize_scores(df_grouped)\n",
    "\n",
    "    df_grouped[\"final_score\"] = df_grouped[\"final_score\"].astype(float)\n",
    "\n",
    "    ratings_matrix = (\n",
    "        df_grouped.pivot_table(\n",
    "            index=\"user_id\", columns=\"movie_id\", values=\"final_score\"\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(float)\n",
    "    )\n",
    "    \n",
    "    return ratings_matrix\n",
    "\n",
    "\n",
    "def __compute_interaction_score(row):\n",
    "    score_fn = INTERACTION_WEIGHTS.get(row[\"interaction_type\"])\n",
    "    if score_fn:\n",
    "        return score_fn(row)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def __add_time_decay(user_interactions: pd.DataFrame) -> pd.DataFrame:\n",
    "    now = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "\n",
    "    user_interactions[\"created_at\"] = pd.to_datetime(\n",
    "        user_interactions[\"created_at\"]\n",
    "    ).dt.tz_localize(\"UTC\", ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "    \n",
    "    user_interactions[\"created_at\"] = user_interactions[\"created_at\"].fillna(now)\n",
    "\n",
    "\n",
    "    user_interactions[\"days_ago\"] = (\n",
    "        now - user_interactions[\"created_at\"]\n",
    "    ).dt.days.clip(lower=0)\n",
    "\n",
    "    decay_rate = 0.97\n",
    "    user_interactions[\"time_decay\"] = decay_rate ** user_interactions[\"days_ago\"]\n",
    "    # user_interactions[\"recency_boost\"] = 2 / (\n",
    "    #     1 + np.exp(user_interactions[\"days_ago\"] / 10)\n",
    "    # )\n",
    "\n",
    "    user_interactions[\"adjusted_score\"] = (\n",
    "        user_interactions[\"interaction_score\"]\n",
    "        * user_interactions[\"time_decay\"]\n",
    "        # * user_interactions[\"recency_boost\"]\n",
    "    )\n",
    "\n",
    "    return user_interactions\n",
    "\n",
    "\n",
    "def __normalize_scores(df_grouped: pd.DataFrame) -> pd.DataFrame:\n",
    "    # df_grouped[\"final_score\"] = (\n",
    "    #     df_grouped.groupby(\"user_id\")[\"final_score\"]\n",
    "    #     .transform(lambda x: 10 * (x - x.min()) / (x.max() - x.min() + 1e-6))\n",
    "    # )\n",
    "    \n",
    "    user_grouped = df_grouped.groupby(\"user_id\")[\"final_score\"]\n",
    "    df_grouped[\"z_score\"] = (\n",
    "        df_grouped[\"final_score\"] - user_grouped.transform(\"mean\")\n",
    "    ) / user_grouped.transform(\"std\").replace(0, 1)\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "ratings_matrix = create_ratings_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation import ratings_matrix as rating_matrix_service\n",
    "from recommendation import recommendation_service\n",
    "import pandas as pd\n",
    "\n",
    "# ratings_matrix = rating_matrix_service.create_ratings_matrix(\"./data/external_interactions_test.csv\")\n",
    "\n",
    "# cbf_df = recommendation_service.get_content_based_filtering_model(ratings_matrix)\n",
    "# cf_df = recommendation_service.get_collaborative_filtering_model(ratings_matrix)\n",
    "\n",
    "hybrid_df = pd.merge(cbf_df, cf_df, on=[\"user_id\", \"movie_id\"], how=\"outer\").fillna(0)\n",
    "\n",
    "# # Combine scores with a weighted average\n",
    "alpha = 0.7\n",
    "hybrid_df[\"final_score\"] = (\n",
    "    alpha * hybrid_df[\"cf_score\"] + (1 - alpha) * hybrid_df[\"content_score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils.utils import user_recommendations\n",
    "from pymongo import UpdateOne\n",
    "import datetime\n",
    "\n",
    "def __store_predictions(predicted_df: pd.DataFrame):\n",
    "    batch_size = 100  # Define the batch size\n",
    "    operations = []\n",
    "    \n",
    "    predicted_df.reset_index\n",
    "\n",
    "    # Process the DataFrame in batches\n",
    "    for start in range(0, len(predicted_df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch: pd.DataFrame = predicted_df.iloc[start:end]\n",
    "        return batch.to_dict(orient=\"records\")\n",
    "        for user_id, row in batch.iterrows():\n",
    "            \n",
    "            return batch, batch.iterrows(), user_id, row\n",
    "\n",
    "            row_df = row.to_frame().T\n",
    "\n",
    "            row_df = row_df.drop(columns=[\"user_id\"], errors=\"ignore\")\n",
    "\n",
    "            # Convert to dictionary format\n",
    "            predictions = row_df.to_dict(orient=\"records\")\n",
    "\n",
    "            operations.append(\n",
    "                UpdateOne(\n",
    "                    {\"user_id\": user_id},\n",
    "                    {\n",
    "                        \"$set\": {\n",
    "                            \"user_id\": user_id,\n",
    "                            \"predictions\": predictions,\n",
    "                            \"last_updated\": datetime.datetime.now(\n",
    "                                tz=datetime.timezone.utc\n",
    "                            ),\n",
    "                        }\n",
    "                    },\n",
    "                    upsert=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Execute the batch of operations\n",
    "        if operations:\n",
    "            return operations\n",
    "            user_recommendations.bulk_write(operations)\n",
    "            operations = []\n",
    "\n",
    "# batch, iter_rows, index, rows = __store_predictions(hybrid_df)\n",
    "dict_rows = __store_predictions(hybrid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation import recommendation_service\n",
    "internal_preds = hybrid_df[hybrid_df['user_id'].str.isnumeric()]\n",
    "\n",
    "batch = [\n",
    "                (\n",
    "                    int(row[\"user_id\"]),\n",
    "                    row[\"movie_id\"],\n",
    "                    round(row[\"final_score\"], 9),\n",
    "                    round(row[\"cf_score\"], 9),\n",
    "                    round(row[\"content_score\"], 9),\n",
    "                    datetime.datetime.now(tz=datetime.timezone.utc),\n",
    "                )\n",
    "                for _, row in internal_preds.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'get_all_user_interactions' executed in 0.05 seconds.\n",
      "Function '__compute_first_interaction_score' executed in 0.10 seconds.\n",
      "Function '__add_time_decay' executed in 0.03 seconds.\n",
      "Function '__normalize_scores' executed in 0.01 seconds.\n",
      "Function 'create_ratings_matrix' executed in 11.18 seconds.\n"
     ]
    }
   ],
   "source": [
    "from recommendation import ratings_matrix as new_rating_matrix\n",
    "\n",
    "ratings_sparse, user_id_lookup, movie_id_lookup = new_rating_matrix.create_ratings_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_index_to_id = {v: k for k, v in user_id_lookup.items()}\n",
    "movie_index_to_id = {v: k for k, v in movie_id_lookup.items()}\n",
    "\n",
    "df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    ratings_sparse,\n",
    "    index=[user_index_to_id[i] for i in range(ratings_sparse.shape[0])],\n",
    "    columns=[movie_index_to_id[i] for i in range(ratings_sparse.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_array\n",
    "\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "user_features = svd.fit_transform(ratings_sparse)\n",
    "movie_features = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = (\n",
    "        user_features @ movie_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = []\n",
    "weights = [6,1,4,10,3,6,2,5,5]\n",
    "\n",
    "for i in range(1, 10):\n",
    "    row = [i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10]\n",
    "    vector.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array(vector) # shape: [n_user_ratings x n_features]\n",
    "weights_array = np.array(weights) # Initially shape: [n_user_ratings x 1] now shape: [1 x n_user_ratings]\n",
    "# user_vector = (vectors * weights).sum(\n",
    "#     axis=0\n",
    "# ) / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n"
     ]
    }
   ],
   "source": [
    "weights_reshaped = weights_array.reshape(-1, 1)\n",
    "print(weights_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector = (vectors * weights_reshaped).sum(\n",
    "        axis=0\n",
    "    ) / weights_reshaped.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_array = (vectors * weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector_re = user_vector.reshape(1, -1)\n",
    "user_vector_re_re = user_vector_re.reshape(1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
